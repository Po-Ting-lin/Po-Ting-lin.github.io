---
layout: post
title: "Global Memory Access - vectorized IO"
date: 2022-10-16
description: "vectorized IO"
img_url: assets/img/post3/post3_cover.png
tags: [memory model,CUDA]
---


### vectorized data type

CUDA has instructions for each thread to do 64 or 128 bit
loads/stores (rather than standard 32 bit transactions).  
Using these data types has advantages:

* At the instruction level  
  A long vector (eg: 128 bits) load or store only requires a single instruction (LDG.E.128) to be issued.  
  The total number of instruction is smaller.
* At the memory controller level  
  A larger net memory throughput per transaction, so 
  fewer transaction requests reduces memory controller contention
  and can produce higher overall memory bandwidth utilization.
  
### Type of vectorized built-in data type

* int2(64 bits), int4(128 bits), float2(64 bits), float4(128 bits), double2(128 bits), etc.

### Simple array addition test

The below example is four vector add into a vector.

```c
__global__ void mulKernel(float* src, float* src2, float* src3, float* src4, float* dst, int dataSize) {
    const int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i >= dataSize) return;
    float a = src[i];
    float b = src2[i];
    float c = src3[i];
    float d = src4[i];
    dst[i] = a + b + c + d;
}

__global__ void mul2Kernel(float2* src, float2* src2, float2* src3, float2* src4, float2* dst, int dataSize) {
    const int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i >= dataSize) return;
    float2 a = src[i];
    float2 b = src2[i];
    float2 c = src3[i];
    float2 d = src4[i];
    dst[i].x = a.x + b.x + c.x + d.x;
    dst[i].y = a.y + b.y + c.y + d.y;
}

__global__ void mul4Kernel(float4* src, float4* src2, float4* src3, float4* src4, float4* dst, int dataSize) {
    const int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i >= dataSize) return;
    float4 a = src[i];
    float4 b = src2[i];
    float4 c = src3[i];
    float4 d = src4[i];
    dst[i].x = a.x + b.x + c.x + d.x;
    dst[i].y = a.y + b.y + c.y + d.y;
    dst[i].z = a.z + b.z + c.z + d.z;
    dst[i].w = a.w + b.w + c.w + d.w;
}
```

mulKernel need 204800 * 4 "LDG.E" instruction" -- the elapsed time: 234 us  
mul2Kernel need 102400 * 4 "LDG.E.64" instruction" -- the elapsed time: 235 us  
mul4Kernel need 51200 * 4 "LDG.E.128 instruction" -- the elapsed time: 233 us  

We can observe there are fewer instructions but not much improvement on the elapsed time in every size of input data.

### Complex multiplication test

```c
__global__ void mulComplexKernel(fComplex* dst, fComplex* src1, fComplex* src2, int dataSize, float c) {
    const int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i >= dataSize) return;
    fComplex a = src1[i];
    fComplex b = src2[i];
    dst[i].x = c * (a.x * b.x - a.y * b.y);
    dst[i].y = c * (a.y * b.x + a.x * b.y);
}

__global__ void mulComplex2Kernel(float2* dst, float2* src1, float2* src2, int dataSize, float c) {
    const int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i >= dataSize) return;
    float2 a = src1[i];
    float2 b = src2[i];
    dst[i].x = c * (a.x * b.x - a.y * b.y);
    dst[i].y = c * (a.y * b.x + a.x * b.y);
}

__global__ void mulComplex4Kernel(float4* dst, float4* src1, float4* src2, int dataSize, float c) {
    const int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i >= dataSize) return;
    float4 a = src1[i];
    float4 b = src2[i];
    dst[i].x = c * (a.x * b.x - a.y * b.y);
    dst[i].y = c * (a.y * b.x + a.x * b.y);
    dst[i].z = c * (a.z * b.z - a.w * b.w);
    dst[i].w = c * (a.w * b.z + a.z * b.w);
}
```

mulComplexKernel need 40960 * 4 "LDG.E" instruction" -- the elapsed time: 60 us  
<img src="/assets/img/post3/mulComplex.jpg" width="800" height="150">  
mulComplex2Kernel need 40960 * 2 "LDG.E.64" instruction" -- the elapsed time: 60 us  
<img src="/assets/img/post3/mulComplex2.jpg" width="800" height="150">  
mulComplex4Kernel need 20480 * 2 "LDG.E.128 instruction" -- the elapsed time: 59 us  
<img src="/assets/img/post3/mulComplex4.jpg" width="800" height="200">  

Even that we can observe more instruction usage in self-defined fComplex than built-in float2.
Still, we observe there are fewer instructions but not much improvement on the elapsed time in every size of input data.

No much difference of device memory throughput between mulComplexKernel and mulComplex4Kernel.
<img src="/assets/img/post3/mulComplexThoughput.jpg" width="800" height="100">  
<img src="/assets/img/post3/mulComplex4Thoughput.jpg" width="800" height="100">  

Here mentioned that they have a great improvement when using float2 and float4 over float
in the two array addition example.  

<a href="https://stackoverflow.com/questions/26676806/efficiency-of-cuda-vector-types-float2-float3-float4">efficiency-of-cuda-vector-types-float2-float3-float4</a>  
However, they used GT540M and Kepler K20c card.
In my testing environment, I used the card with Ampere architecture, which is late in several generations.

Here also mentioned this issue.  
<a href="https://forums.developer.nvidia.com/t/float4-bandwidth-advantages-over-plain-float1/62799">float4-bandwidth-advantages-over-plain-float1</a>  
![https://forums.developer.nvidia.com/t/float4-bandwidth-advantages-over-plain-float1/62799]()  
It seems the reason is that the new micro-architecture changes in how the memory hierarchy was implemented in hardware.
Therefore, the new micro-architecture must optimize the global memory access in certain degree.
The NVIDIA cuda documentation also cannot find the part to emphasize "vectorized data type".

### Conclusion

There still need to verity if we can measure the improvement in the micro-architecture before Maxwell, but
I need to get the card first.
Despite that, we also need to keep in mind this optimization skill.
