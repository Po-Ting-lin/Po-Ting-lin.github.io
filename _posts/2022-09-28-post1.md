---
layout: post
title: "Case Review of Bank Conflict"
date: 2022-09-28
description: "Case review of shared memory bank conflict"
img_url: assets/img/post2/before_pad_crop.png
tags: [memory model,CUDA]
---


### Memory Banks

To achieve high memory bandwidth, shared memory is divided into 32 memory module, called banks,
which can be accessed simultaneously.   
The addresses of shared memory are mapped to different banks in different patterns.   

### Bank Conflict

If multiple addresses in a shared memory request fall into the same memory bank, a bank conflict occurs.
It splits a request as many conflict-free transactions, decreasing the effective bandwidth.

<img src="/assets/img/post2/bank_conflict.png" width="700" height="400">

## Instance
### Baseline Code

In the following case, the below kernel calculate the 2D scan of the input source image.   
At first, it loads all the data into shared memory.  

Second, it scans in the x-axis by scanInclusiveWarp function.
Each warp accesses 32 continuous shared memory address.
Each address is loaded and stored in separate 32 banks. 
Therefore, no bank conflict occurs.

Then, it scans in the y-axis by scanInclusiveWarp function.
Each warp accesses the address which is the same bank.
Therefore, there are 32-way bank conflict occurs.

In the case of 4-byte(32-bit) shared memory access mode:   
```c
Bank Index = (byte address / 4) % bank size 
```

The baseline Kernel:
```c
__global__ void blockScan(uchar* src, uintll* dst, uintll* colReduced, uintll* rowReduced, int nx, int ny) {
    __shared__ uintll smem[WARP_SIZE][WARP_SIZE];
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    uintll last_sum;
    uintll value;

    uintll v = src[y * nx + x];
    smem[threadIdx.y][threadIdx.x] = v * v;
    __syncthreads();

    // row scan
    last_sum = blockIdx.x != 0 ? colReduced[(blockIdx.x - 1) * ny + y] : 0;
    value = smem[threadIdx.y][threadIdx.x];
    smem[threadIdx.y][threadIdx.x] = scanInclusiveWarp(value) + last_sum;
    __syncthreads();

    // col scan
    last_sum = blockIdx.y != 0 ? rowReduced[(blockIdx.y - 1) * nx + x] : 0;
    value = smem[threadIdx.x][threadIdx.y];
    smem[threadIdx.x][threadIdx.y] = scanInclusiveWarp(value);
    __syncthreads();

    dst[y * nx + x] = smem[threadIdx.y][threadIdx.x] + last_sum;
}
```

We can observe the bank conflict shown in the Nsight Compute:
<img src="/assets/img/post2/before_pad_crop.png" width="700" height="150">

## Shared Memory Padding


When scanning in the y-axis, the first warp access y-axis shared memory,
which causes 32-way bank conflict. A simple trick can let the access pattern change.

Here is an example:   
<img src="/assets/img/post2/shared_memory_padding.jpg" width="600" height="500">

Thus, the simple padding can avoid serious bank conflict:
```
__global__ void blockScan(uchar* src, uintll* dst, uintll* colReduced, uintll* rowReduced, int nx, int ny) {
    __shared__ uintll smem[WARP_SIZE][WARP_SIZE + 1];
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    uintll last_sum;
    uintll value;

    uintll v = src[y * nx + x];
    smem[threadIdx.y][threadIdx.x] = v * v;
    __syncthreads();

    // row scan
    last_sum = blockIdx.x != 0 ? colReduced[(blockIdx.x - 1) * ny + y] : 0;
    value = smem[threadIdx.y][threadIdx.x];
    smem[threadIdx.y][threadIdx.x] = scanInclusiveWarp(value) + last_sum;
    __syncthreads();

    // col scan
    last_sum = blockIdx.y != 0 ? rowReduced[(blockIdx.y - 1) * nx + x] : 0;
    value = smem[threadIdx.x][threadIdx.y];
    smem[threadIdx.x][threadIdx.y] = scanInclusiveWarp(value);
    __syncthreads();

    dst[y * nx + x] = smem[threadIdx.y][threadIdx.x] + last_sum;
}
```

The improvement can be easily observed in the Nsight Compute:
<img src="/assets/img/post2/after_pad_crop.png" width="700" height="150">

The elapsed time is improved from 91.33 us to 63.36 us (x1.4 speedup).   
When using shared memory, we must pay attention on the access pattern on shared memory.
